{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch BeautifulSoup requests"
      ],
      "metadata": {
        "id": "Wgl799RWABNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38fe621-c2f9-43dd-b779-a3a52499ff85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting BeautifulSoup\n",
            "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2HQqG07NVW0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "yKX-oXravGdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_text (URL):\n",
        "  response = requests.get(URL)\n",
        "  response.raise_for_status()\n",
        "  html = response.text\n",
        "  soup = BeautifulSoup(html , 'html.parser')\n",
        "\n",
        "  for script_or_style in soup([\"script\", \"style\"]):\n",
        "    script_or_style.decompose()\n",
        "\n",
        "  text = soup.get_text()\n",
        "  lines = text.splitlines()\n",
        "  chunks = []\n",
        "  for line in lines:\n",
        "    parts = line.split()\n",
        "    clean_line = ' '.join(parts)\n",
        "    if clean_line:\n",
        "      chunks.append(clean_line)\n",
        "  clean_text = '\\n'.join(chunks)\n",
        "\n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "v2FLFDIlAauR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer , BartForConditionalGeneration\n",
        "def summarize_text(ARTICLE , model_name = \"facebook/bart-large-cnn\"):\n",
        "  tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "  model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "  inputs = tokenizer(ARTICLE, return_tensors=\"pt\", max_length = 1024, truncation=True)\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
        "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return summary"
      ],
      "metadata": {
        "id": "yT-7nJAdPenb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://freedium.cfd/https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122\""
      ],
      "metadata": {
        "id": "Av5L5M0wUczT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE = fetch_text(URL)\n",
        "summary = summarize_text(ARTICLE)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0mB7uGFwvN4",
        "outputId": "cc02655c-a2f5-43dd-aefc-a1d38c7a5099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI GPT-3: Language Models are Few-Shot Learners | by Soheil Tehranipour | in Analytics Vidhya - Freedium. We regret to inform you that our account on BuyMeACoffee has been suspended due to a violation of their terms of service. We are transitioning to Patreon, a platform that aligns with our values and offers us the freedom to share our work with you. Please join us on Patreon and continue to support our endeavors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHnIPgpLw7pI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}